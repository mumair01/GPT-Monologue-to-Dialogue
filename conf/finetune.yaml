# Configure the finetune experiment
hydra:
  run:
    dir: ${env.paths.root}/data/results/finetune/${finetune.model.model_name}_${dataset.name}/${now:%Y-%m-%d_%H-%M-%S}/
  job:
    chdir: True


finetune:
  model:
    model_name : gpt2-large
    model_checkpoint : gpt2-large
    tokenizer_checkpoint : gpt2
    tokenizer_additional_special_tokens :
      - <SP1> # Speaker 1 token
      - <SP2> # Speaker 2 token
      - <START> # Conversation start token
      - <END> # Conversation end token
    tokenizer_pad_token : "<PAD>"
    tokenizer_eos_token : "<|endoftext|>"
  training:
    data_block_size : 128
    num_train_epochs : 30
    per_device_train_batch_size : 8
    per_device_eval_batch_size : 8
    warmup_steps : 300

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "\n",
    "Proof of concept notebook for finetuning GPT using HuggingFace and Torch and \n",
    "is similar to notebook 2.0. However, in this notebook, we replace \n",
    "TextDataset with custom chunking of the datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download libraries for environment. \n",
    "\n",
    "import sys \n",
    "import os \n",
    "\n",
    "# Env. vars to check if the notebook is running on colab, kaggle etc. \n",
    "IS_COLAB = \"google.colab\" in sys.modules \n",
    "IS_KAGGLE = \"kaggle_secrets\" in sys.modules \n",
    "IS_LOCAL = not (IS_COLAB or IS_KAGGLE)\n",
    "\n",
    "if IS_COLAB:\n",
    "    # Install the packages \n",
    "    %pip install -q -U tensorflow-addons\n",
    "    %pip install -q -U transformers\n",
    "    %pip install -q -U datasets\n",
    "    print(\"You can safely ignore the package incompatibility errors.\")\n",
    "    # Mount the drive \n",
    "    from google.colab import drive \n",
    "    drive.mount(\"/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "\n",
    "import random \n",
    "import shutil \n",
    "# Scikit-Learn â‰¥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "\n",
    "# Pytorch imports \n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Others \n",
    "import glob \n",
    "\n",
    "# Transformers \n",
    "import transformers \n",
    "from transformers import TextDataset,DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments,AutoModelWithLMHead\n",
    "from transformers import AutoTokenizer\n",
    "import datasets \n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --  Set environment global vars. \n",
    "\n",
    "# Shared env. vars. \n",
    "GLOBAL_SEED = 42 \n",
    "IS_CUDA_ENV = torch.cuda.is_available()\n",
    "GLOBAL_DEVICE = torch.device('cuda') if IS_CUDA_ENV else torch.device('cpu')\n",
    "SET_SEED = True # If true, sets the global seeds for this notebook. \n",
    "\n",
    "if IS_LOCAL:\n",
    "    LIMITED_RESOURCES = not IS_CUDA_ENV\n",
    "    SMALL_DATASET_SIZE = 500\n",
    "\n",
    "if IS_COLAB:\n",
    "    LIMITED_RESOURCES = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring env. \n",
    "if SET_SEED:\n",
    "    # to make this notebook's output stable across runs\n",
    "    np.random.seed(GLOBAL_SEED) \n",
    "    torch.manual_seed(GLOBAL_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/muhammadumair/Documents/Repositories/mumair01-repos/GPT-Monologue-to-Dialogue/data/processed/in_conversation_corpus_poc'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Project Paths\n",
    "NOTEBOOK_NAME = \"gpt_finetune_custom_dataset_poc\"\n",
    "PROJECT_ROOT_DIR = \"/Users/muhammadumair/Documents/Repositories/mumair01-repos/GPT-Monologue-to-Dialogue\" \n",
    "# --- Input data dirs. \n",
    "DATASET_NAME = \"in_conversation_corpus_poc\"\n",
    "DATASET_TYPE = \"csv\"\n",
    "PROCESSED_DATA_DIR = os.path.join(PROJECT_ROOT_DIR,\"data\", \"processed\", DATASET_NAME)\n",
    "\n",
    "# --- Result dirs. \n",
    "SAVE_MODEL_DIR = os.path.join(PROJECT_ROOT_DIR,\"models\",NOTEBOOK_NAME)\n",
    "REPORTS_DIR = os.path.join(PROJECT_ROOT_DIR,\"reports\",NOTEBOOK_NAME)\n",
    "\n",
    "os.makedirs(REPORTS_DIR,exist_ok=True)\n",
    "os.makedirs(SAVE_MODEL_DIR,exist_ok=True)\n",
    "PROCESSED_DATA_DIR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning HuggingFace GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NOTE: The below should be the same in the dataset - assuming there are 2 speakers! \n",
    "SPEAKER_1_TOKEN = \"<SP1>\"\n",
    "SPEAKER_2_TOKEN = \"<SP2>\"\n",
    "CONV_START_TOKEN = \"<START>\"\n",
    "CONV_END_TOKEN = \"<END>\"\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "EOS_TOKEN = \"<|endoftext|>\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(REPORTS_DIR, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer vars. \n",
    "\n",
    "TOKENIZER_CHECKPOINT = \"gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: In the next cell, we were adding the additional tokens as new \n",
    "# tokens. However, it seems that the special tokens are not masked by the data \n",
    "# collator, which might lead to weird training results. \n",
    "# Therefore, loading a basic version of the tokenizer. \n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    TOKENIZER_CHECKPOINT\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the tokenizer with special tokens defined. \n",
    "# tokenizer = AutoTokenizer.from_pretrained(\n",
    "#     TOKENIZER_CHECKPOINT, \n",
    "#     pad_token = PAD_TOKEN, \n",
    "#     eos_token = EOS_TOKEN, \n",
    "#     additional_special_tokens=(\n",
    "#         SPEAKER_1_TOKEN, SPEAKER_2_TOKEN, CONV_START_TOKEN, \n",
    "#         CONV_END_TOKEN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/Users/muhammadumair/Documents/Repositories/mumair01-repos/GPT-Monologue-to-Dialogue/models/gpt_finetune_custom_dataset_poc/tokenizer_config.json',\n",
       " '/Users/muhammadumair/Documents/Repositories/mumair01-repos/GPT-Monologue-to-Dialogue/models/gpt_finetune_custom_dataset_poc/special_tokens_map.json',\n",
       " '/Users/muhammadumair/Documents/Repositories/mumair01-repos/GPT-Monologue-to-Dialogue/models/gpt_finetune_custom_dataset_poc/vocab.json',\n",
       " '/Users/muhammadumair/Documents/Repositories/mumair01-repos/GPT-Monologue-to-Dialogue/models/gpt_finetune_custom_dataset_poc/merges.txt',\n",
       " '/Users/muhammadumair/Documents/Repositories/mumair01-repos/GPT-Monologue-to-Dialogue/models/gpt_finetune_custom_dataset_poc/added_tokens.json',\n",
       " '/Users/muhammadumair/Documents/Repositories/mumair01-repos/GPT-Monologue-to-Dialogue/models/gpt_finetune_custom_dataset_poc/tokenizer.json')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the tokenizer after adding new tokens \n",
    "tokenizer.save_pretrained(SAVE_MODEL_DIR) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='/Users/muhammadumair/Documents/Repositories/mumair01-repos/GPT-Monologue-to-Dialogue/models/gpt_finetune_custom_dataset_poc', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'})"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = tokenizer.from_pretrained(SAVE_MODEL_DIR)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the maximum content size of the tokenizer \n",
    "# NOTE: This can be changed depending on the amount of GPU memory that is available. \n",
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': '/Users/muhammadumair/Documents/Repositories/mumair01-repos/GPT-Monologue-to-Dialogue/data/processed/in_conversation_corpus_poc/train.csv',\n",
       " 'validation': '/Users/muhammadumair/Documents/Repositories/mumair01-repos/GPT-Monologue-to-Dialogue/data/processed/in_conversation_corpus_poc/validation.csv'}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_paths = glob.glob(\"{}/*.csv\".format(PROCESSED_DATA_DIR))\n",
    "dataset_paths = {os.path.splitext(os.path.basename(p))[0] : p for p in dataset_paths}\n",
    "# Only keep the required keys / verify that they exist.\n",
    "dataset_paths = {k : dataset_paths[k] for k in ('train','validation')} \n",
    "dataset_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: The TextDataset is deprecated - please see the next cell for the updated \n",
    "# method of creating the text dataset. Note that this is still included in \n",
    "# case the new approach does not work. \n",
    "\n",
    "#####  UNCOMMENT IF NEEDED ######\n",
    "\n",
    "# train_dataset = TextDataset(\n",
    "#     tokenizer=tokenizer,\n",
    "#     file_path=dataset_paths[\"train\"],\n",
    "#     block_size=128)\n",
    "# validation_dataset = TextDataset(\n",
    "#     tokenizer=tokenizer,\n",
    "#     file_path=dataset_paths[\"validation\"],\n",
    "#     block_size=128)\n",
    "\n",
    "##################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-ec291898ba3afb33\n",
      "Reusing dataset csv (/Users/muhammadumair/.cache/huggingface/datasets/csv/default-ec291898ba3afb33/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12b500a47817483a9d228da1397fd9b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loading text dataset using new method \n",
    "if LIMITED_RESOURCES:\n",
    "    train_dataset, validation_dataset = load_dataset(DATASET_TYPE, data_files=dataset_paths, \n",
    "    split=[\n",
    "        datasets.ReadInstruction('train', from_=0, to=SMALL_DATASET_SIZE, unit='abs'),\n",
    "        datasets.ReadInstruction('validation', from_=0, to=SMALL_DATASET_SIZE, unit='abs')])\n",
    "    dataset = DatasetDict({\n",
    "        \"train\" : train_dataset, \n",
    "        \"validation\" : validation_dataset\n",
    "    })\n",
    "else:\n",
    "    dataset = load_dataset(DATASET_TYPE, data_files=dataset_paths)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0', 'convID', 'Utterance'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Unnamed: 0', 'convID', 'Utterance'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each row in this dataset is an utterance. \n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Preprocessing and Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER_BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b6f24752d354547ad182c59dc6531a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "314714e9d79d49d8811e0e43d5e7492b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Once loaded, the dataset needs to be processed \n",
    "def tokenize_fn(tokenizer):\n",
    "    return lambda data: tokenizer(data[\"Utterance\"], truncation=True) \n",
    "# NOTE: The batched with map allows the tokenize_fn to be applied to multiple \n",
    "# batched of the input data at the same time for faster processing - but does not \n",
    "# affect the tokenizer results themselves. \n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_fn(tokenizer), batched=True, remove_columns=[\"Unnamed: 0\",\"convID\",\"Utterance\"])\n",
    "# tokenized_datasets = dataset.map(\n",
    "#     tokenize_fn(tokenizer), batched=True, batch_size=TOKENIZER_BATCH_SIZE, remove_columns=[\"Unnamed: 0\",\"convID\",\"Utterance\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 500\n",
       "})"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizing adds the input_ids and attention_mask. \n",
    "tokenized_datasets['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [50259, 8788, 220, 50259], 'attention_mask': [1, 1, 1, 1]}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each item in the tokenized dataset is simply that utterance\n",
    "tokenized_datasets['train'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Utterance index 0 : Utterance length: 1'\n",
      "'>>> Utterance index 1 : Utterance length: 6'\n",
      "'>>> Utterance index 2 : Utterance length: 18'\n",
      "'>>> Utterance index 3 : Utterance length: 8'\n",
      "'>>> Utterance index 4 : Utterance length: 16'\n",
      "'>>> Utterance index 5 : Utterance length: 18'\n",
      "'>>> Utterance index 6 : Utterance length: 27'\n",
      "'>>> Utterance index 7 : Utterance length: 28'\n",
      "'>>> Utterance index 8 : Utterance length: 4'\n",
      "'>>> Utterance index 9 : Utterance length: 14'\n",
      "'>>> Utterance index 10 : Utterance length: 4'\n",
      "'>>> Utterance index 11 : Utterance length: 14'\n",
      "'>>> Utterance index 12 : Utterance length: 24'\n",
      "'>>> Utterance index 13 : Utterance length: 8'\n",
      "'>>> Utterance index 14 : Utterance length: 24'\n",
      "'>>> Utterance index 15 : Utterance length: 9'\n",
      "'>>> Utterance index 16 : Utterance length: 22'\n",
      "'>>> Utterance index 17 : Utterance length: 12'\n",
      "'>>> Utterance index 18 : Utterance length: 7'\n",
      "'>>> Utterance index 19 : Utterance length: 10'\n",
      "'>>> Utterance index 20 : Utterance length: 31'\n",
      "'>>> Utterance index 21 : Utterance length: 11'\n",
      "'>>> Utterance index 22 : Utterance length: 8'\n",
      "'>>> Utterance index 23 : Utterance length: 8'\n",
      "'>>> Utterance index 24 : Utterance length: 5'\n",
      "'>>> Utterance index 25 : Utterance length: 14'\n",
      "'>>> Utterance index 26 : Utterance length: 46'\n",
      "'>>> Utterance index 27 : Utterance length: 4'\n",
      "'>>> Utterance index 28 : Utterance length: 13'\n",
      "'>>> Utterance index 29 : Utterance length: 36'\n",
      "'>>> Utterance index 30 : Utterance length: 29'\n",
      "'>>> Utterance index 31 : Utterance length: 5'\n",
      "'>>> Utterance index 32 : Utterance length: 22'\n",
      "'>>> Utterance index 33 : Utterance length: 11'\n",
      "'>>> Utterance index 34 : Utterance length: 4'\n",
      "'>>> Utterance index 35 : Utterance length: 8'\n",
      "'>>> Utterance index 36 : Utterance length: 17'\n",
      "'>>> Utterance index 37 : Utterance length: 5'\n",
      "'>>> Utterance index 38 : Utterance length: 4'\n",
      "'>>> Utterance index 39 : Utterance length: 5'\n",
      "'>>> Utterance index 40 : Utterance length: 9'\n",
      "'>>> Utterance index 41 : Utterance length: 31'\n",
      "'>>> Utterance index 42 : Utterance length: 40'\n",
      "'>>> Utterance index 43 : Utterance length: 4'\n",
      "'>>> Utterance index 44 : Utterance length: 4'\n",
      "'>>> Utterance index 45 : Utterance length: 12'\n",
      "'>>> Utterance index 46 : Utterance length: 22'\n",
      "'>>> Utterance index 47 : Utterance length: 8'\n",
      "'>>> Utterance index 48 : Utterance length: 27'\n",
      "'>>> Utterance index 49 : Utterance length: 18'\n",
      "'>>> Utterance index 50 : Utterance length: 13'\n",
      "'>>> Utterance index 51 : Utterance length: 17'\n",
      "'>>> Utterance index 52 : Utterance length: 9'\n",
      "'>>> Utterance index 53 : Utterance length: 6'\n",
      "'>>> Utterance index 54 : Utterance length: 6'\n",
      "'>>> Utterance index 55 : Utterance length: 47'\n",
      "'>>> Utterance index 56 : Utterance length: 4'\n",
      "'>>> Utterance index 57 : Utterance length: 28'\n",
      "'>>> Utterance index 58 : Utterance length: 8'\n",
      "'>>> Utterance index 59 : Utterance length: 7'\n",
      "'>>> Utterance index 60 : Utterance length: 18'\n",
      "'>>> Utterance index 61 : Utterance length: 4'\n",
      "'>>> Utterance index 62 : Utterance length: 18'\n",
      "'>>> Utterance index 63 : Utterance length: 13'\n",
      "'>>> Utterance index 64 : Utterance length: 11'\n",
      "'>>> Utterance index 65 : Utterance length: 4'\n",
      "'>>> Utterance index 66 : Utterance length: 8'\n",
      "'>>> Utterance index 67 : Utterance length: 4'\n",
      "'>>> Utterance index 68 : Utterance length: 4'\n",
      "'>>> Utterance index 69 : Utterance length: 13'\n",
      "'>>> Utterance index 70 : Utterance length: 4'\n",
      "'>>> Utterance index 71 : Utterance length: 4'\n",
      "'>>> Utterance index 72 : Utterance length: 9'\n",
      "'>>> Utterance index 73 : Utterance length: 9'\n",
      "'>>> Utterance index 74 : Utterance length: 10'\n",
      "'>>> Utterance index 75 : Utterance length: 9'\n",
      "'>>> Utterance index 76 : Utterance length: 5'\n",
      "'>>> Utterance index 77 : Utterance length: 12'\n",
      "'>>> Utterance index 78 : Utterance length: 9'\n",
      "'>>> Utterance index 79 : Utterance length: 12'\n",
      "'>>> Utterance index 80 : Utterance length: 6'\n",
      "'>>> Utterance index 81 : Utterance length: 8'\n",
      "'>>> Utterance index 82 : Utterance length: 13'\n",
      "'>>> Utterance index 83 : Utterance length: 5'\n",
      "'>>> Utterance index 84 : Utterance length: 11'\n",
      "'>>> Utterance index 85 : Utterance length: 4'\n",
      "'>>> Utterance index 86 : Utterance length: 15'\n",
      "'>>> Utterance index 87 : Utterance length: 4'\n",
      "'>>> Utterance index 88 : Utterance length: 29'\n",
      "'>>> Utterance index 89 : Utterance length: 21'\n",
      "'>>> Utterance index 90 : Utterance length: 11'\n",
      "'>>> Utterance index 91 : Utterance length: 4'\n",
      "'>>> Utterance index 92 : Utterance length: 10'\n",
      "'>>> Utterance index 93 : Utterance length: 6'\n",
      "'>>> Utterance index 94 : Utterance length: 5'\n",
      "'>>> Utterance index 95 : Utterance length: 8'\n",
      "'>>> Utterance index 96 : Utterance length: 4'\n",
      "'>>> Utterance index 97 : Utterance length: 14'\n",
      "'>>> Utterance index 98 : Utterance length: 4'\n",
      "'>>> Utterance index 99 : Utterance length: 29'\n",
      "'>>> Utterance index 100 : Utterance length: 6'\n",
      "'>>> Utterance index 101 : Utterance length: 16'\n",
      "'>>> Utterance index 102 : Utterance length: 4'\n",
      "'>>> Utterance index 103 : Utterance length: 62'\n",
      "'>>> Utterance index 104 : Utterance length: 6'\n",
      "'>>> Utterance index 105 : Utterance length: 35'\n",
      "'>>> Utterance index 106 : Utterance length: 10'\n",
      "'>>> Utterance index 107 : Utterance length: 6'\n",
      "'>>> Utterance index 108 : Utterance length: 4'\n",
      "'>>> Utterance index 109 : Utterance length: 36'\n",
      "'>>> Utterance index 110 : Utterance length: 6'\n",
      "'>>> Utterance index 111 : Utterance length: 16'\n",
      "'>>> Utterance index 112 : Utterance length: 19'\n",
      "'>>> Utterance index 113 : Utterance length: 7'\n",
      "'>>> Utterance index 114 : Utterance length: 6'\n",
      "'>>> Utterance index 115 : Utterance length: 21'\n",
      "'>>> Utterance index 116 : Utterance length: 4'\n",
      "'>>> Utterance index 117 : Utterance length: 63'\n",
      "'>>> Utterance index 118 : Utterance length: 7'\n",
      "'>>> Utterance index 119 : Utterance length: 8'\n",
      "'>>> Utterance index 120 : Utterance length: 5'\n",
      "'>>> Utterance index 121 : Utterance length: 9'\n",
      "'>>> Utterance index 122 : Utterance length: 5'\n",
      "'>>> Utterance index 123 : Utterance length: 30'\n",
      "'>>> Utterance index 124 : Utterance length: 5'\n",
      "'>>> Utterance index 125 : Utterance length: 30'\n",
      "'>>> Utterance index 126 : Utterance length: 9'\n",
      "'>>> Utterance index 127 : Utterance length: 18'\n",
      "'>>> Utterance index 128 : Utterance length: 4'\n",
      "'>>> Utterance index 129 : Utterance length: 131'\n",
      "'>>> Utterance index 130 : Utterance length: 8'\n",
      "'>>> Utterance index 131 : Utterance length: 5'\n",
      "'>>> Utterance index 132 : Utterance length: 10'\n",
      "'>>> Utterance index 133 : Utterance length: 23'\n",
      "'>>> Utterance index 134 : Utterance length: 9'\n",
      "'>>> Utterance index 135 : Utterance length: 12'\n",
      "'>>> Utterance index 136 : Utterance length: 8'\n",
      "'>>> Utterance index 137 : Utterance length: 7'\n",
      "'>>> Utterance index 138 : Utterance length: 4'\n",
      "'>>> Utterance index 139 : Utterance length: 53'\n",
      "'>>> Utterance index 140 : Utterance length: 4'\n",
      "'>>> Utterance index 141 : Utterance length: 6'\n",
      "'>>> Utterance index 142 : Utterance length: 85'\n",
      "'>>> Utterance index 143 : Utterance length: 4'\n",
      "'>>> Utterance index 144 : Utterance length: 42'\n",
      "'>>> Utterance index 145 : Utterance length: 4'\n",
      "'>>> Utterance index 146 : Utterance length: 4'\n",
      "'>>> Utterance index 147 : Utterance length: 41'\n",
      "'>>> Utterance index 148 : Utterance length: 4'\n",
      "'>>> Utterance index 149 : Utterance length: 8'\n",
      "'>>> Utterance index 150 : Utterance length: 9'\n",
      "'>>> Utterance index 151 : Utterance length: 9'\n",
      "'>>> Utterance index 152 : Utterance length: 39'\n",
      "'>>> Utterance index 153 : Utterance length: 9'\n",
      "'>>> Utterance index 154 : Utterance length: 24'\n",
      "'>>> Utterance index 155 : Utterance length: 4'\n",
      "'>>> Utterance index 156 : Utterance length: 18'\n",
      "'>>> Utterance index 157 : Utterance length: 20'\n",
      "'>>> Utterance index 158 : Utterance length: 31'\n",
      "'>>> Utterance index 159 : Utterance length: 6'\n",
      "'>>> Utterance index 160 : Utterance length: 5'\n",
      "'>>> Utterance index 161 : Utterance length: 26'\n",
      "'>>> Utterance index 162 : Utterance length: 4'\n",
      "'>>> Utterance index 163 : Utterance length: 11'\n",
      "'>>> Utterance index 164 : Utterance length: 9'\n",
      "'>>> Utterance index 165 : Utterance length: 6'\n",
      "'>>> Utterance index 166 : Utterance length: 21'\n",
      "'>>> Utterance index 167 : Utterance length: 15'\n",
      "'>>> Utterance index 168 : Utterance length: 9'\n",
      "'>>> Utterance index 169 : Utterance length: 5'\n",
      "'>>> Utterance index 170 : Utterance length: 4'\n",
      "'>>> Utterance index 171 : Utterance length: 8'\n",
      "'>>> Utterance index 172 : Utterance length: 13'\n",
      "'>>> Utterance index 173 : Utterance length: 8'\n",
      "'>>> Utterance index 174 : Utterance length: 83'\n",
      "'>>> Utterance index 175 : Utterance length: 5'\n",
      "'>>> Utterance index 176 : Utterance length: 16'\n",
      "'>>> Utterance index 177 : Utterance length: 14'\n",
      "'>>> Utterance index 178 : Utterance length: 8'\n",
      "'>>> Utterance index 179 : Utterance length: 12'\n",
      "'>>> Utterance index 180 : Utterance length: 5'\n",
      "'>>> Utterance index 181 : Utterance length: 60'\n",
      "'>>> Utterance index 182 : Utterance length: 4'\n",
      "'>>> Utterance index 183 : Utterance length: 24'\n",
      "'>>> Utterance index 184 : Utterance length: 7'\n",
      "'>>> Utterance index 185 : Utterance length: 4'\n",
      "'>>> Utterance index 186 : Utterance length: 31'\n",
      "'>>> Utterance index 187 : Utterance length: 5'\n",
      "'>>> Utterance index 188 : Utterance length: 27'\n",
      "'>>> Utterance index 189 : Utterance length: 20'\n",
      "'>>> Utterance index 190 : Utterance length: 7'\n",
      "'>>> Utterance index 191 : Utterance length: 13'\n",
      "'>>> Utterance index 192 : Utterance length: 5'\n",
      "'>>> Utterance index 193 : Utterance length: 6'\n",
      "'>>> Utterance index 194 : Utterance length: 7'\n",
      "'>>> Utterance index 195 : Utterance length: 5'\n",
      "'>>> Utterance index 196 : Utterance length: 12'\n",
      "'>>> Utterance index 197 : Utterance length: 35'\n",
      "'>>> Utterance index 198 : Utterance length: 21'\n",
      "'>>> Utterance index 199 : Utterance length: 13'\n",
      "'>>> Utterance index 200 : Utterance length: 5'\n",
      "'>>> Utterance index 201 : Utterance length: 17'\n",
      "'>>> Utterance index 202 : Utterance length: 6'\n",
      "'>>> Utterance index 203 : Utterance length: 17'\n",
      "'>>> Utterance index 204 : Utterance length: 9'\n",
      "'>>> Utterance index 205 : Utterance length: 11'\n",
      "'>>> Utterance index 206 : Utterance length: 26'\n",
      "'>>> Utterance index 207 : Utterance length: 10'\n",
      "'>>> Utterance index 208 : Utterance length: 4'\n",
      "'>>> Utterance index 209 : Utterance length: 8'\n",
      "'>>> Utterance index 210 : Utterance length: 5'\n",
      "'>>> Utterance index 211 : Utterance length: 6'\n",
      "'>>> Utterance index 212 : Utterance length: 11'\n",
      "'>>> Utterance index 213 : Utterance length: 15'\n",
      "'>>> Utterance index 214 : Utterance length: 17'\n",
      "'>>> Utterance index 215 : Utterance length: 4'\n",
      "'>>> Utterance index 216 : Utterance length: 12'\n",
      "'>>> Utterance index 217 : Utterance length: 10'\n",
      "'>>> Utterance index 218 : Utterance length: 6'\n",
      "'>>> Utterance index 219 : Utterance length: 15'\n",
      "'>>> Utterance index 220 : Utterance length: 16'\n",
      "'>>> Utterance index 221 : Utterance length: 4'\n",
      "'>>> Utterance index 222 : Utterance length: 14'\n",
      "'>>> Utterance index 223 : Utterance length: 9'\n",
      "'>>> Utterance index 224 : Utterance length: 13'\n",
      "'>>> Utterance index 225 : Utterance length: 67'\n",
      "'>>> Utterance index 226 : Utterance length: 10'\n",
      "'>>> Utterance index 227 : Utterance length: 18'\n",
      "'>>> Utterance index 228 : Utterance length: 5'\n",
      "'>>> Utterance index 229 : Utterance length: 23'\n",
      "'>>> Utterance index 230 : Utterance length: 8'\n",
      "'>>> Utterance index 231 : Utterance length: 5'\n",
      "'>>> Utterance index 232 : Utterance length: 17'\n",
      "'>>> Utterance index 233 : Utterance length: 8'\n",
      "'>>> Utterance index 234 : Utterance length: 14'\n",
      "'>>> Utterance index 235 : Utterance length: 11'\n",
      "'>>> Utterance index 236 : Utterance length: 6'\n",
      "'>>> Utterance index 237 : Utterance length: 7'\n",
      "'>>> Utterance index 238 : Utterance length: 4'\n",
      "'>>> Utterance index 239 : Utterance length: 14'\n",
      "'>>> Utterance index 240 : Utterance length: 35'\n",
      "'>>> Utterance index 241 : Utterance length: 10'\n",
      "'>>> Utterance index 242 : Utterance length: 13'\n",
      "'>>> Utterance index 243 : Utterance length: 8'\n",
      "'>>> Utterance index 244 : Utterance length: 20'\n",
      "'>>> Utterance index 245 : Utterance length: 4'\n",
      "'>>> Utterance index 246 : Utterance length: 9'\n",
      "'>>> Utterance index 247 : Utterance length: 4'\n",
      "'>>> Utterance index 248 : Utterance length: 18'\n",
      "'>>> Utterance index 249 : Utterance length: 13'\n",
      "'>>> Utterance index 250 : Utterance length: 9'\n",
      "'>>> Utterance index 251 : Utterance length: 5'\n",
      "'>>> Utterance index 252 : Utterance length: 5'\n",
      "'>>> Utterance index 253 : Utterance length: 11'\n",
      "'>>> Utterance index 254 : Utterance length: 19'\n",
      "'>>> Utterance index 255 : Utterance length: 17'\n",
      "'>>> Utterance index 256 : Utterance length: 8'\n",
      "'>>> Utterance index 257 : Utterance length: 15'\n",
      "'>>> Utterance index 258 : Utterance length: 11'\n",
      "'>>> Utterance index 259 : Utterance length: 32'\n",
      "'>>> Utterance index 260 : Utterance length: 5'\n",
      "'>>> Utterance index 261 : Utterance length: 25'\n",
      "'>>> Utterance index 262 : Utterance length: 5'\n",
      "'>>> Utterance index 263 : Utterance length: 26'\n",
      "'>>> Utterance index 264 : Utterance length: 4'\n",
      "'>>> Utterance index 265 : Utterance length: 29'\n",
      "'>>> Utterance index 266 : Utterance length: 27'\n",
      "'>>> Utterance index 267 : Utterance length: 32'\n",
      "'>>> Utterance index 268 : Utterance length: 17'\n",
      "'>>> Utterance index 269 : Utterance length: 8'\n",
      "'>>> Utterance index 270 : Utterance length: 30'\n",
      "'>>> Utterance index 271 : Utterance length: 4'\n",
      "'>>> Utterance index 272 : Utterance length: 4'\n",
      "'>>> Utterance index 273 : Utterance length: 15'\n",
      "'>>> Utterance index 274 : Utterance length: 4'\n",
      "'>>> Utterance index 275 : Utterance length: 17'\n",
      "'>>> Utterance index 276 : Utterance length: 20'\n",
      "'>>> Utterance index 277 : Utterance length: 11'\n",
      "'>>> Utterance index 278 : Utterance length: 11'\n",
      "'>>> Utterance index 279 : Utterance length: 8'\n",
      "'>>> Utterance index 280 : Utterance length: 7'\n",
      "'>>> Utterance index 281 : Utterance length: 9'\n",
      "'>>> Utterance index 282 : Utterance length: 14'\n",
      "'>>> Utterance index 283 : Utterance length: 8'\n",
      "'>>> Utterance index 284 : Utterance length: 67'\n",
      "'>>> Utterance index 285 : Utterance length: 4'\n",
      "'>>> Utterance index 286 : Utterance length: 9'\n",
      "'>>> Utterance index 287 : Utterance length: 67'\n",
      "'>>> Utterance index 288 : Utterance length: 9'\n",
      "'>>> Utterance index 289 : Utterance length: 5'\n",
      "'>>> Utterance index 290 : Utterance length: 7'\n",
      "'>>> Utterance index 291 : Utterance length: 21'\n",
      "'>>> Utterance index 292 : Utterance length: 12'\n",
      "'>>> Utterance index 293 : Utterance length: 9'\n",
      "'>>> Utterance index 294 : Utterance length: 6'\n",
      "'>>> Utterance index 295 : Utterance length: 5'\n",
      "'>>> Utterance index 296 : Utterance length: 30'\n",
      "'>>> Utterance index 297 : Utterance length: 16'\n",
      "'>>> Utterance index 298 : Utterance length: 11'\n",
      "'>>> Utterance index 299 : Utterance length: 33'\n",
      "'>>> Utterance index 300 : Utterance length: 35'\n",
      "'>>> Utterance index 301 : Utterance length: 4'\n",
      "'>>> Utterance index 302 : Utterance length: 8'\n",
      "'>>> Utterance index 303 : Utterance length: 8'\n",
      "'>>> Utterance index 304 : Utterance length: 4'\n",
      "'>>> Utterance index 305 : Utterance length: 4'\n",
      "'>>> Utterance index 306 : Utterance length: 5'\n",
      "'>>> Utterance index 307 : Utterance length: 5'\n",
      "'>>> Utterance index 308 : Utterance length: 18'\n",
      "'>>> Utterance index 309 : Utterance length: 14'\n",
      "'>>> Utterance index 310 : Utterance length: 11'\n",
      "'>>> Utterance index 311 : Utterance length: 8'\n",
      "'>>> Utterance index 312 : Utterance length: 6'\n",
      "'>>> Utterance index 313 : Utterance length: 5'\n",
      "'>>> Utterance index 314 : Utterance length: 5'\n",
      "'>>> Utterance index 315 : Utterance length: 8'\n",
      "'>>> Utterance index 316 : Utterance length: 14'\n",
      "'>>> Utterance index 317 : Utterance length: 8'\n",
      "'>>> Utterance index 318 : Utterance length: 37'\n",
      "'>>> Utterance index 319 : Utterance length: 5'\n",
      "'>>> Utterance index 320 : Utterance length: 18'\n",
      "'>>> Utterance index 321 : Utterance length: 7'\n",
      "'>>> Utterance index 322 : Utterance length: 9'\n",
      "'>>> Utterance index 323 : Utterance length: 4'\n",
      "'>>> Utterance index 324 : Utterance length: 90'\n",
      "'>>> Utterance index 325 : Utterance length: 7'\n",
      "'>>> Utterance index 326 : Utterance length: 4'\n",
      "'>>> Utterance index 327 : Utterance length: 7'\n",
      "'>>> Utterance index 328 : Utterance length: 5'\n",
      "'>>> Utterance index 329 : Utterance length: 17'\n",
      "'>>> Utterance index 330 : Utterance length: 10'\n",
      "'>>> Utterance index 331 : Utterance length: 25'\n",
      "'>>> Utterance index 332 : Utterance length: 1'\n",
      "'>>> Utterance index 333 : Utterance length: 1'\n",
      "'>>> Utterance index 334 : Utterance length: 5'\n",
      "'>>> Utterance index 335 : Utterance length: 20'\n",
      "'>>> Utterance index 336 : Utterance length: 5'\n",
      "'>>> Utterance index 337 : Utterance length: 9'\n",
      "'>>> Utterance index 338 : Utterance length: 9'\n",
      "'>>> Utterance index 339 : Utterance length: 6'\n",
      "'>>> Utterance index 340 : Utterance length: 6'\n",
      "'>>> Utterance index 341 : Utterance length: 23'\n",
      "'>>> Utterance index 342 : Utterance length: 17'\n",
      "'>>> Utterance index 343 : Utterance length: 5'\n",
      "'>>> Utterance index 344 : Utterance length: 4'\n",
      "'>>> Utterance index 345 : Utterance length: 7'\n",
      "'>>> Utterance index 346 : Utterance length: 7'\n",
      "'>>> Utterance index 347 : Utterance length: 5'\n",
      "'>>> Utterance index 348 : Utterance length: 5'\n",
      "'>>> Utterance index 349 : Utterance length: 5'\n",
      "'>>> Utterance index 350 : Utterance length: 11'\n",
      "'>>> Utterance index 351 : Utterance length: 7'\n",
      "'>>> Utterance index 352 : Utterance length: 4'\n",
      "'>>> Utterance index 353 : Utterance length: 6'\n",
      "'>>> Utterance index 354 : Utterance length: 13'\n",
      "'>>> Utterance index 355 : Utterance length: 13'\n",
      "'>>> Utterance index 356 : Utterance length: 9'\n",
      "'>>> Utterance index 357 : Utterance length: 12'\n",
      "'>>> Utterance index 358 : Utterance length: 6'\n",
      "'>>> Utterance index 359 : Utterance length: 8'\n",
      "'>>> Utterance index 360 : Utterance length: 11'\n",
      "'>>> Utterance index 361 : Utterance length: 9'\n",
      "'>>> Utterance index 362 : Utterance length: 15'\n",
      "'>>> Utterance index 363 : Utterance length: 13'\n",
      "'>>> Utterance index 364 : Utterance length: 5'\n",
      "'>>> Utterance index 365 : Utterance length: 5'\n",
      "'>>> Utterance index 366 : Utterance length: 5'\n",
      "'>>> Utterance index 367 : Utterance length: 4'\n",
      "'>>> Utterance index 368 : Utterance length: 7'\n",
      "'>>> Utterance index 369 : Utterance length: 9'\n",
      "'>>> Utterance index 370 : Utterance length: 5'\n",
      "'>>> Utterance index 371 : Utterance length: 11'\n",
      "'>>> Utterance index 372 : Utterance length: 8'\n",
      "'>>> Utterance index 373 : Utterance length: 5'\n",
      "'>>> Utterance index 374 : Utterance length: 17'\n",
      "'>>> Utterance index 375 : Utterance length: 5'\n",
      "'>>> Utterance index 376 : Utterance length: 23'\n",
      "'>>> Utterance index 377 : Utterance length: 14'\n",
      "'>>> Utterance index 378 : Utterance length: 5'\n",
      "'>>> Utterance index 379 : Utterance length: 9'\n",
      "'>>> Utterance index 380 : Utterance length: 12'\n",
      "'>>> Utterance index 381 : Utterance length: 14'\n",
      "'>>> Utterance index 382 : Utterance length: 8'\n",
      "'>>> Utterance index 383 : Utterance length: 25'\n",
      "'>>> Utterance index 384 : Utterance length: 5'\n",
      "'>>> Utterance index 385 : Utterance length: 30'\n",
      "'>>> Utterance index 386 : Utterance length: 41'\n",
      "'>>> Utterance index 387 : Utterance length: 8'\n",
      "'>>> Utterance index 388 : Utterance length: 14'\n",
      "'>>> Utterance index 389 : Utterance length: 20'\n",
      "'>>> Utterance index 390 : Utterance length: 6'\n",
      "'>>> Utterance index 391 : Utterance length: 14'\n",
      "'>>> Utterance index 392 : Utterance length: 15'\n",
      "'>>> Utterance index 393 : Utterance length: 6'\n",
      "'>>> Utterance index 394 : Utterance length: 4'\n",
      "'>>> Utterance index 395 : Utterance length: 19'\n",
      "'>>> Utterance index 396 : Utterance length: 6'\n",
      "'>>> Utterance index 397 : Utterance length: 26'\n",
      "'>>> Utterance index 398 : Utterance length: 4'\n",
      "'>>> Utterance index 399 : Utterance length: 6'\n",
      "'>>> Utterance index 400 : Utterance length: 73'\n",
      "'>>> Utterance index 401 : Utterance length: 43'\n",
      "'>>> Utterance index 402 : Utterance length: 16'\n",
      "'>>> Utterance index 403 : Utterance length: 12'\n",
      "'>>> Utterance index 404 : Utterance length: 7'\n",
      "'>>> Utterance index 405 : Utterance length: 10'\n",
      "'>>> Utterance index 406 : Utterance length: 10'\n",
      "'>>> Utterance index 407 : Utterance length: 18'\n",
      "'>>> Utterance index 408 : Utterance length: 16'\n",
      "'>>> Utterance index 409 : Utterance length: 5'\n",
      "'>>> Utterance index 410 : Utterance length: 17'\n",
      "'>>> Utterance index 411 : Utterance length: 15'\n",
      "'>>> Utterance index 412 : Utterance length: 7'\n",
      "'>>> Utterance index 413 : Utterance length: 63'\n",
      "'>>> Utterance index 414 : Utterance length: 14'\n",
      "'>>> Utterance index 415 : Utterance length: 53'\n",
      "'>>> Utterance index 416 : Utterance length: 9'\n",
      "'>>> Utterance index 417 : Utterance length: 5'\n",
      "'>>> Utterance index 418 : Utterance length: 4'\n",
      "'>>> Utterance index 419 : Utterance length: 33'\n",
      "'>>> Utterance index 420 : Utterance length: 8'\n",
      "'>>> Utterance index 421 : Utterance length: 13'\n",
      "'>>> Utterance index 422 : Utterance length: 6'\n",
      "'>>> Utterance index 423 : Utterance length: 31'\n",
      "'>>> Utterance index 424 : Utterance length: 4'\n",
      "'>>> Utterance index 425 : Utterance length: 8'\n",
      "'>>> Utterance index 426 : Utterance length: 37'\n",
      "'>>> Utterance index 427 : Utterance length: 6'\n",
      "'>>> Utterance index 428 : Utterance length: 8'\n",
      "'>>> Utterance index 429 : Utterance length: 28'\n",
      "'>>> Utterance index 430 : Utterance length: 15'\n",
      "'>>> Utterance index 431 : Utterance length: 19'\n",
      "'>>> Utterance index 432 : Utterance length: 7'\n",
      "'>>> Utterance index 433 : Utterance length: 8'\n",
      "'>>> Utterance index 434 : Utterance length: 10'\n",
      "'>>> Utterance index 435 : Utterance length: 12'\n",
      "'>>> Utterance index 436 : Utterance length: 7'\n",
      "'>>> Utterance index 437 : Utterance length: 12'\n",
      "'>>> Utterance index 438 : Utterance length: 4'\n",
      "'>>> Utterance index 439 : Utterance length: 4'\n",
      "'>>> Utterance index 440 : Utterance length: 7'\n",
      "'>>> Utterance index 441 : Utterance length: 9'\n",
      "'>>> Utterance index 442 : Utterance length: 6'\n",
      "'>>> Utterance index 443 : Utterance length: 7'\n",
      "'>>> Utterance index 444 : Utterance length: 9'\n",
      "'>>> Utterance index 445 : Utterance length: 11'\n",
      "'>>> Utterance index 446 : Utterance length: 6'\n",
      "'>>> Utterance index 447 : Utterance length: 4'\n",
      "'>>> Utterance index 448 : Utterance length: 6'\n",
      "'>>> Utterance index 449 : Utterance length: 16'\n",
      "'>>> Utterance index 450 : Utterance length: 4'\n",
      "'>>> Utterance index 451 : Utterance length: 10'\n",
      "'>>> Utterance index 452 : Utterance length: 12'\n",
      "'>>> Utterance index 453 : Utterance length: 9'\n",
      "'>>> Utterance index 454 : Utterance length: 11'\n",
      "'>>> Utterance index 455 : Utterance length: 4'\n",
      "'>>> Utterance index 456 : Utterance length: 20'\n",
      "'>>> Utterance index 457 : Utterance length: 14'\n",
      "'>>> Utterance index 458 : Utterance length: 5'\n",
      "'>>> Utterance index 459 : Utterance length: 11'\n",
      "'>>> Utterance index 460 : Utterance length: 8'\n",
      "'>>> Utterance index 461 : Utterance length: 9'\n",
      "'>>> Utterance index 462 : Utterance length: 5'\n",
      "'>>> Utterance index 463 : Utterance length: 5'\n",
      "'>>> Utterance index 464 : Utterance length: 11'\n",
      "'>>> Utterance index 465 : Utterance length: 13'\n",
      "'>>> Utterance index 466 : Utterance length: 7'\n",
      "'>>> Utterance index 467 : Utterance length: 45'\n",
      "'>>> Utterance index 468 : Utterance length: 8'\n",
      "'>>> Utterance index 469 : Utterance length: 4'\n",
      "'>>> Utterance index 470 : Utterance length: 6'\n",
      "'>>> Utterance index 471 : Utterance length: 4'\n",
      "'>>> Utterance index 472 : Utterance length: 12'\n",
      "'>>> Utterance index 473 : Utterance length: 5'\n",
      "'>>> Utterance index 474 : Utterance length: 4'\n",
      "'>>> Utterance index 475 : Utterance length: 14'\n",
      "'>>> Utterance index 476 : Utterance length: 10'\n",
      "'>>> Utterance index 477 : Utterance length: 12'\n",
      "'>>> Utterance index 478 : Utterance length: 11'\n",
      "'>>> Utterance index 479 : Utterance length: 7'\n",
      "'>>> Utterance index 480 : Utterance length: 9'\n",
      "'>>> Utterance index 481 : Utterance length: 10'\n",
      "'>>> Utterance index 482 : Utterance length: 10'\n",
      "'>>> Utterance index 483 : Utterance length: 10'\n",
      "'>>> Utterance index 484 : Utterance length: 1'\n",
      "'>>> Utterance index 485 : Utterance length: 1'\n",
      "'>>> Utterance index 486 : Utterance length: 12'\n",
      "'>>> Utterance index 487 : Utterance length: 7'\n",
      "'>>> Utterance index 488 : Utterance length: 4'\n",
      "'>>> Utterance index 489 : Utterance length: 6'\n",
      "'>>> Utterance index 490 : Utterance length: 11'\n",
      "'>>> Utterance index 491 : Utterance length: 4'\n",
      "'>>> Utterance index 492 : Utterance length: 7'\n",
      "'>>> Utterance index 493 : Utterance length: 4'\n",
      "'>>> Utterance index 494 : Utterance length: 4'\n",
      "'>>> Utterance index 495 : Utterance length: 6'\n",
      "'>>> Utterance index 496 : Utterance length: 12'\n",
      "'>>> Utterance index 497 : Utterance length: 4'\n",
      "'>>> Utterance index 498 : Utterance length: 8'\n",
      "'>>> Utterance index 499 : Utterance length: 12'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Slicing produces a list of lists for each feature\n",
    "if LIMITED_RESOURCES:\n",
    "    tokenized_samples = tokenized_datasets[\"train\"][:SMALL_DATASET_SIZE]\n",
    "else:\n",
    "    tokenized_samples = tokenized_datasets[\"train\"][:-1]\n",
    "for idx, sample in enumerate(tokenized_samples[\"input_ids\"]):\n",
    "    print(f\"'>>> Utterance index {idx} : Utterance length: {len(sample)}'\")\n",
    "tokenized_samples.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3674500b682d49db9e5c7fc6fc7fe05c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6d8ed529cde417e882d5e86abf89730",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 52\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 47\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can manually create chunks of data of the dataset using dict. comprehension. \n",
    "# NOTE: We need to decide whether to pad the last chunk or discard it. \n",
    "\n",
    "def chunk_tokenized_samples(tokenized_samples,chunk_size=128):\n",
    "    # Concatenate all the utterances \n",
    "    keys =  ('input_ids','attention_mask')\n",
    "    concatenated_examples = {k : sum(tokenized_samples[k],[]) for k in keys}\n",
    "    total_length = len(concatenated_examples[keys[0]])\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    # NOTE: This method is discarding the last chunk. \n",
    "    chunks = {\n",
    "        k : [concatenated_examples[k][i:i+ chunk_size] for i in range(0, total_length,chunk_size)]  for k in keys\n",
    "    }\n",
    "    return chunks\n",
    "    \n",
    "\n",
    "lm_datasets = tokenized_datasets.map(chunk_tokenized_samples,batched=True)\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the number of tokens in each chunk - which should be equal to chunk size. \n",
    "len(lm_datasets['train'][0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<START><SP1> this is fun <SP1><SP2> this is freaking me out cause i feel like im looking at like my reflection <SP2><SP1> were gonna become rock stars <SP1><SP2> i feel like im like in the future this is creeping me out <SP2><SP1> woah its like it looks like it could be a mirror but its not <SP1><SP2> i feel like i like it is a mirror like im really disturbed i dont think were either like in the state to handle <SP2><SP1> sorry yeah i know i literally have a migraine and im and like ew okay what was i saying before we walked in here <SP1><SP2> stats <SP2><SP1> stats'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the result of decoding the first CHUNK_SIZE tokens in the dataset. \n",
    "tokenizer.decode(lm_datasets['train'][0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data collator, which is responsible for creating batches from the\n",
    "# datasets during training.\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, \n",
    "    mlm=False, \n",
    "    return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model / Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CHECKPOINT = \"distilgpt2\" if LIMITED_RESOURCES else \"gpt2-large\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model \n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_CHECKPOINT, \n",
    "    pad_token_id = tokenizer.pad_token_id, \n",
    "    eos_token_id = tokenizer.eos_token_id\n",
    ")\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining training arguments \n",
    "training_args = TrainingArguments(\n",
    "        output_dir=SAVE_MODEL_DIR,\n",
    "        overwrite_output_dir=False,\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        eval_steps=200,\n",
    "        save_steps=400,\n",
    "        warmup_steps=300,\n",
    "        prediction_loss_only=True,\n",
    "        evaluation_strategy='epoch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the trainer \n",
    "# NOTE: Trainer should automatically put the model and dataset to GPU \n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args, \n",
    "    data_collator=data_collator, \n",
    "    train_dataset=lm_datasets['train'], \n",
    "    eval_dataset=lm_datasets['validation']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear caches before training \n",
    "torch.cuda.empty_cache()\n",
    "gc.collect() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This will probably not run locally. \n",
    "trainer.train() \n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('gpt_proj')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8226f9d8233b889d814e65b178e86bbabe6dbb1167ee6423a6522d592207fe9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

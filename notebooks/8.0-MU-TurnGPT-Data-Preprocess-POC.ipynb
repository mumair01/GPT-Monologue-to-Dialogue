{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proof of concept notebook that formats ICC and Speaker Identity datasets \n",
    "for use with TurnGPT. \n",
    "\n",
    "This is because TurnGPT expects the data to be in a different format compared\n",
    "with GPT-2. \n",
    "\n",
    "NOTE: The point of this document is to quickly generate the data - and it \n",
    "may be a bit hacky / need refactoring when the project is complete. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download libraries for environment. \n",
    "\n",
    "import sys \n",
    "import os \n",
    "\n",
    "# Env. vars to check if the notebook is running on colab, kaggle etc. \n",
    "IS_COLAB = \"google.colab\" in sys.modules \n",
    "IS_KAGGLE = \"kaggle_secrets\" in sys.modules \n",
    "IS_LOCAL = not (IS_COLAB or IS_KAGGLE)\n",
    "\n",
    "if IS_COLAB:\n",
    "    # Install the packages \n",
    "    %pip install -q -U tensorflow-addons\n",
    "    %pip install -q -U transformers\n",
    "    %pip install -q -U datasets\n",
    "    print(\"You can safely ignore the package incompatibility errors.\")\n",
    "    # Mount the drive \n",
    "    from google.colab import drive \n",
    "    drive.mount(\"/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "\n",
    "import random \n",
    "import shutil \n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "\n",
    "# Pytorch imports \n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Others \n",
    "import glob \n",
    "\n",
    "# Transformers \n",
    "import transformers \n",
    "from transformers import TextDataset,DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments,AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import datasets \n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --  Set environment global vars. \n",
    "\n",
    "# Shared env. vars. \n",
    "GLOBAL_SEED = 42 \n",
    "IS_CUDA_ENV = torch.cuda.is_available()\n",
    "GLOBAL_DEVICE = torch.device('cuda') if IS_CUDA_ENV else torch.device('cpu')\n",
    "SET_SEED = True # If true, sets the global seeds for this notebook. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring env. \n",
    "if SET_SEED:\n",
    "    # to make this notebook's output stable across runs\n",
    "    np.random.seed(GLOBAL_SEED) \n",
    "    torch.manual_seed(GLOBAL_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project Paths\n",
    "NOTEBOOK_NAME = \"8.0-MU-TurnGPT-Data-Preprocess-POC\"\n",
    "PROJECT_ROOT_DIR = \"/Users/muhammadumair/Documents/Repositories/mumair01-repos/GPT-Monologue-to-Dialogue\" \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning - In Conversation Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project Paths\n",
    "# --- Input data dirs. \n",
    "DATASET_NAME = \"ICC/julia_dissertation_turngpt\"\n",
    "DATASET_TYPE = \"csv\"\n",
    "PROCESSED_DATA_DIR = os.path.join(PROJECT_ROOT_DIR,\"data\",\"datasets\", \"processed\", DATASET_NAME)\n",
    "RAW_DATA_DIR = os.path.join(PROJECT_ROOT_DIR,\"data\",\"datasets\", \"raw\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "from copy import deepcopy \n",
    "from sklearn.utils import shuffle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Assumptions about the previous data:\n",
    "    1. Has been previous pre-processed by Julia. \n",
    "    2. Contains start / end tokens at the start and end of each .cha file. \n",
    "    3. Each line format is SPX\\t <text> \\tSPX \n",
    "    4. There are only two speakers per conversation.\n",
    "'''\n",
    "\n",
    "def preprocess_huggingface_icc(cha_paths, seed=GLOBAL_SEED):\n",
    "    \"\"\"Creates a dataset dataframe from Julia's processed .cha files.\"\"\"\n",
    "    cha_paths = deepcopy(cha_paths)\n",
    "    cha_paths = shuffle(cha_paths,random_state=seed)\n",
    "    pbar = tqdm(desc=\"Preprocessing ICC conversations\", total=len(cha_paths))\n",
    "    data = [] \n",
    "    for i in range(len(cha_paths)):\n",
    "        with open(cha_paths[i],'r') as f:\n",
    "            # Read all lines as a list \n",
    "            conv_name = os.path.splitext(os.path.basename(cha_paths[i]))[0]\n",
    "            conv = f.readlines()\n",
    "            for j in range(len(conv)):\n",
    "                target_str = conv[j].strip() \n",
    "                split_toks = re.split(r\"\\. |\\?|\\t+\", target_str)\n",
    "                split_toks = [tok for tok in split_toks if len(tok) > 0] \n",
    "                # Remove all punctuation and lowercase all \n",
    "                split_toks = [re.sub(r'[^\\w\\s]', '', tok).lower() for tok in split_toks]\n",
    "                # Remove any double whitespaces \n",
    "                split_toks = [re.sub(' +', ' ', tok).lower() for tok in split_toks]\n",
    "                # Removing existing speaker tokens.\n",
    "                for sp_label in (r'sp1',r'sp2','start','end'):\n",
    "                    split_toks = [tok for tok in split_toks if not re.match(sp_label, tok) ]\n",
    "                if len(split_toks) == 3:\n",
    "                    split_toks = [\" \".join(split_toks)]\n",
    "                data.extend([(conv_name,i, tok) for tok in split_toks])\n",
    "        pbar.update()\n",
    "    dataset_df = pd.DataFrame(data, columns=[\"convName\",\"convID\", \"Utterance\"])    \n",
    "    return dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_save_cha(cha_paths,dataset_name, output_dir):\n",
    "    os.makedirs(output_dir,exist_ok=True)\n",
    "    dataset_df = preprocess_huggingface_icc(cha_paths, seed=GLOBAL_SEED)\n",
    "    dataset_df.to_csv(os.path.join(output_dir,dataset_name)+\".csv\")\n",
    "    # Save the dataframe as a text file as well \n",
    "    # NOTE: This is important to make sure that TextDataset can read these \n",
    "    # files during finetuning. \n",
    "    with open(os.path.join(output_dir,dataset_name)+\".txt\",\"w\") as f:\n",
    "        f.writelines(\"\\n\".join(dataset_df[\"Utterance\"]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/muhammadumair/Documents/Repositories/mumair01-repos/GPT-Monologue-to-Dialogue/data/datasets/raw'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RAW_DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Change with the directory containing the cha files. \n",
    "\n",
    "DIR_PATH = os.path.join(\n",
    "    RAW_DATA_DIR,\"ICC/julia_finetune_experiments/28_train_14_test_set/test\")\n",
    "NAME = \"test\"\n",
    "OUT_DIR = os.path.join(PROCESSED_DATA_DIR,\"28_train_14_test_set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "cha_paths = glob.glob(\"{}/*.cha\".format(DIR_PATH))\n",
    "print(len(cha_paths))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing ICC conversations: 100%|██████████| 14/14 [00:00<00:00, 54.42it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocess_and_save_cha(cha_paths,NAME,OUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference - Speaker Identity Stims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"speaker_identity_stims_turngpt\"\n",
    "DATASET_TYPE = \"csv\"\n",
    "PROCESSED_DATA_DIR = os.path.join(PROJECT_ROOT_DIR,\"data\",\"datasets\", \"processed\", DATASET_NAME)\n",
    "RAW_DATA_DIR = os.path.join(PROJECT_ROOT_DIR,\"data\",\"datasets\", \"raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/muhammadumair/Documents/Repositories/mumair01-repos/GPT-Monologue-to-Dialogue/data/datasets/processed/speaker_identity_stims_turngpt'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DIR_PATH = os.path.join(\n",
    "    RAW_DATA_DIR,\"speaker_identity_stims\")\n",
    "NAME = \"test\"\n",
    "OUT_DIR = PROCESSED_DATA_DIR\n",
    "OUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750\n"
     ]
    }
   ],
   "source": [
    "cha_paths = glob.glob(\"{}/*.cha\".format(DIR_PATH))\n",
    "print(len(cha_paths))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Assumptions about the previous data:\n",
    "    1. Each line is one turn in format: <Speaker label>: <Utterance>\n",
    "    2. There are only two speakers per conversation.\n",
    "    3. Any lines starting with @ are metadata and ignored. \n",
    "'''\n",
    "\n",
    "def preprocess_lena_stimuli(cha_paths,num_speakers=2, seed=GLOBAL_SEED):\n",
    "    \"\"\"Creates a dataset dataframe from Julia's processed .cha files.\"\"\"\n",
    "    cha_paths = deepcopy(cha_paths)\n",
    "    cha_paths = shuffle(cha_paths,random_state=seed)\n",
    "    pbar = tqdm(desc=\"Preprocessing Lena's stimuli\", total=len(cha_paths))\n",
    "    data = [] \n",
    "    for i in range(len(cha_paths)):\n",
    "        with open(cha_paths[i],'r') as f:\n",
    "            # Read all lines as a list \n",
    "            conv_name = os.path.splitext(os.path.basename(cha_paths[i]))[0]\n",
    "            conv = f.readlines()\n",
    "            # Ignore / Remove all lines that start with comment marker. \n",
    "            conv = [line for line in conv if line[0] != \"@\"]\n",
    "            # data.append((conv_name,i,CONV_START_TOK))\n",
    "            for j in range(len(conv)):\n",
    "                # Splitting utterance by punctuation i.e., punctuated substrings \n",
    "                # will appear as separate lines. \n",
    "                target_str = conv[j].strip() \n",
    "                # NOTE: Splitting by colon for the speaker ID. \n",
    "                split_toks = re.split(r\"\\. |\\?|\\t+|:\", target_str)\n",
    "                split_toks = [tok.strip() for tok in split_toks if len(tok) > 0] \n",
    "                # Remove all punctuation and lowercase all \n",
    "                split_toks = [re.sub(r'[^\\w\\s]', '', tok).lower() for tok in split_toks]\n",
    "                # Remove any double whitespaces \n",
    "                split_toks = [re.sub(' +', ' ', tok).lower() for tok in split_toks]\n",
    "                # Removing existing speaker tokens to add the ones needed by the model. \n",
    "                # NOTE: Assuming that speaker ids start from 1.\n",
    "                split_toks = split_toks[1]\n",
    "                # for speaker_id in range(num_speakers+1):\n",
    "                #     split_toks = [tok for tok in split_toks if not re.match(\"sp{}\".format(speaker_id),tok) ]\n",
    "                # Add the trailing speaker token \n",
    "                # split_toks.append(split_toks[0])\n",
    "                    # split_toks = list( \" \".join(split_toks))\n",
    "                data.append((conv_name, i, split_toks))\n",
    "                # data.extend([(conv_name,i, tok) for tok in split_toks])\n",
    "        # data.append((conv_name,i,CONV_END_TOK))\n",
    "        pbar.update()\n",
    "    dataset_df = pd.DataFrame(data, columns=[\"convName\",\"convID\", \"Utterance\"])    \n",
    "    return dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing Lena's stimuli: 100%|██████████| 750/750 [00:00<00:00, 7964.84it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_df = preprocess_lena_stimuli(cha_paths, seed=GLOBAL_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>convName</th>\n",
       "      <th>convID</th>\n",
       "      <th>Utterance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19bd</td>\n",
       "      <td>0</td>\n",
       "      <td>i havent told you this story yet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19bd</td>\n",
       "      <td>0</td>\n",
       "      <td>why not</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9ba</td>\n",
       "      <td>1</td>\n",
       "      <td>you handled that situation so calmly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9ba</td>\n",
       "      <td>1</td>\n",
       "      <td>well done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11c</td>\n",
       "      <td>2</td>\n",
       "      <td>you parked in my spot again</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>77c</td>\n",
       "      <td>747</td>\n",
       "      <td>i am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>45ba</td>\n",
       "      <td>748</td>\n",
       "      <td>i have the secret santa gift in my bag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>45ba</td>\n",
       "      <td>748</td>\n",
       "      <td>dont look</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>1c</td>\n",
       "      <td>749</td>\n",
       "      <td>ive been trying to unscrew this bolt for fifte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>1c</td>\n",
       "      <td>749</td>\n",
       "      <td>want help</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     convName  convID                                          Utterance\n",
       "0        19bd       0                   i havent told you this story yet\n",
       "1        19bd       0                                            why not\n",
       "2         9ba       1               you handled that situation so calmly\n",
       "3         9ba       1                                          well done\n",
       "4         11c       2                        you parked in my spot again\n",
       "...       ...     ...                                                ...\n",
       "1495      77c     747                                               i am\n",
       "1496     45ba     748             i have the secret santa gift in my bag\n",
       "1497     45ba     748                                          dont look\n",
       "1498       1c     749  ive been trying to unscrew this bolt for fifte...\n",
       "1499       1c     749                                          want help\n",
       "\n",
       "[1500 rows x 3 columns]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df.sort_values(by=['convID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset to file\n",
    "dataset_df.to_csv(os.path.join(PROCESSED_DATA_DIR,\"test\")+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('gpt_proj_turn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9147fc0b06ce0e31e9567362322f61cf8f6b68299a29ba983f10190fe3a4e71a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

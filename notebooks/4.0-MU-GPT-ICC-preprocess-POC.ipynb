{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Proof of concept notebook for obtaining and preprocessing In-Conversation-Corpus data for finetuning HuggingFace GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download libraries for environment. \n",
    "\n",
    "import sys \n",
    "import os \n",
    "\n",
    "# Env. vars to check if the notebook is running on colab, kaggle etc. \n",
    "IS_COLAB = \"google.colab\" in sys.modules \n",
    "IS_KAGGLE = \"kaggle_secrets\" in sys.modules \n",
    "IS_LOCAL = not (IS_COLAB or IS_KAGGLE)\n",
    "\n",
    "if IS_COLAB:\n",
    "    # Install the packages \n",
    "    %pip install -q -U tensorflow-addons\n",
    "    %pip install -q -U transformers\n",
    "    %pip install -q -U datasets\n",
    "    print(\"You can safely ignore the package incompatibility errors.\")\n",
    "    # Mount the drive \n",
    "    from google.colab import drive \n",
    "    drive.mount(\"/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "\n",
    "import random \n",
    "import shutil \n",
    "# Scikit-Learn â‰¥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "\n",
    "# Pytorch imports \n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Others \n",
    "import glob \n",
    "\n",
    "# Transformers \n",
    "import transformers \n",
    "from transformers import TextDataset,DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments,AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import datasets \n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --  Set environment global vars. \n",
    "\n",
    "# Shared env. vars. \n",
    "GLOBAL_SEED = 42 \n",
    "IS_CUDA_ENV = torch.cuda.is_available()\n",
    "GLOBAL_DEVICE = torch.device('cuda') if IS_CUDA_ENV else torch.device('cpu')\n",
    "SET_SEED = True # If true, sets the global seeds for this notebook. \n",
    "\n",
    "if IS_LOCAL:\n",
    "    SMALL_DATASET = True if not IS_CUDA_ENV else False # Use a small dataset if no cuda env. \n",
    "    SMALL_DATASET_SIZE = 3 \n",
    "\n",
    "\n",
    "if IS_COLAB:\n",
    "    SMALL_DATASET = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring env. \n",
    "if SET_SEED:\n",
    "    # to make this notebook's output stable across runs\n",
    "    np.random.seed(GLOBAL_SEED) \n",
    "    torch.manual_seed(GLOBAL_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project Paths\n",
    "NOTEBOOK_NAME = \"preprocess_icc_gpt\"\n",
    "PROJECT_ROOT_DIR = \"/Users/muhammadumair/Documents/Repositories/mumair01-repos/GPT-Monologue-to-Dialogue\" \n",
    "# --- Input data dirs. \n",
    "DATASET_NAME = \"in_conversation_corpus_poc\"\n",
    "DATASET_TYPE = \"csv\"\n",
    "PROCESSED_DATA_DIR = os.path.join(PROJECT_ROOT_DIR,\"data\", \"processed\", DATASET_NAME)\n",
    "RAW_DATA_DIR = os.path.join(PROJECT_ROOT_DIR,\"data\", \"raw\", DATASET_NAME, \"train\")\n",
    "\n",
    "# --- Result dirs. \n",
    "# NOTE: The model dir will have to change depending on where the models are stored. \n",
    "REPORTS_DIR = os.path.join(PROJECT_ROOT_DIR,\"reports\",NOTEBOOK_NAME)\n",
    "\n",
    "os.makedirs(REPORTS_DIR,exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-Conversation Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Processed CHAT data \n",
    "\n",
    "In this section, we process .cha files and convert them to .csv files for use with the HuggingFace GPT model. \n",
    "\n",
    "Note that these files were previously processed to include speaker labels at the start and end of a turn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "from copy import deepcopy \n",
    "from sklearn.utils import shuffle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the start and end tokens \n",
    "SPEAKER_TOK = \"<SP{}>\" \n",
    "CONV_START_TOK = \"<START>\"\n",
    "CONV_END_TOK = \"<END>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Each of these files represents a single conversation. \n",
    "cha_paths = glob.glob(\"{}/*.cha\".format(RAW_DATA_DIR))\n",
    "\n",
    "if SMALL_DATASET:\n",
    "    cha_paths = cha_paths[:SMALL_DATASET_SIZE]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Assumptions about the previous data:\n",
    "    1. Has been previous pre-processed by Julia. \n",
    "    2. Contains start / end tokens at the start and end of each .cha file. \n",
    "    3. Each line format is SPX\\t <text> \\tSPX \n",
    "    4. There are only two speakers per conversation.\n",
    "'''\n",
    "\n",
    "def preprocess_huggingface_icc(cha_paths, seed=GLOBAL_SEED):\n",
    "    \"\"\"Creates a dataset dataframe from Julia's processed .cha files.\"\"\"\n",
    "    cha_paths = deepcopy(cha_paths)\n",
    "    cha_paths = shuffle(cha_paths,random_state=seed)\n",
    "    pbar = tqdm(desc=\"Preprocessing ICC conversations\", total=len(cha_paths))\n",
    "    data = [] \n",
    "    for i in range(len(cha_paths)):\n",
    "        with open(cha_paths[i],'r') as f:\n",
    "            # Read all lines as a list \n",
    "            conv = f.readlines()\n",
    "            for j in range(len(conv)):\n",
    "                target_str = conv[j].strip() \n",
    "                split_toks = re.split(r\"\\. |\\?|\\t+\", target_str)\n",
    "                split_toks = [tok for tok in split_toks if len(tok) > 0] \n",
    "                # Remove all punctuation and lowercase all \n",
    "                split_toks = [re.sub(r'[^\\w\\s]', '', tok).lower() for tok in split_toks]\n",
    "                # Remove any double whitespaces \n",
    "                split_toks = [re.sub(' +', ' ', tok).lower() for tok in split_toks]\n",
    "                # Removing existing speaker tokens to add the ones needed by the model. \n",
    "                split_toks = [SPEAKER_TOK.format(\"1\") if re.match(r\"(sp1)\", tok) else tok for tok in split_toks]\n",
    "                split_toks = [SPEAKER_TOK.format(\"2\") if re.match(r\"(sp2)\", tok) else tok for tok in split_toks]\n",
    "                split_toks = [CONV_START_TOK if re.match('start',tok)  else tok for tok in split_toks]\n",
    "                split_toks = [CONV_END_TOK if re.match('end',tok) else tok for tok in split_toks] \n",
    "                if len(split_toks) == 3:\n",
    "                    split_toks = [\" \".join(split_toks)]\n",
    "                    # split_toks = list( \" \".join(split_toks))\n",
    "                data.extend([(i, tok) for tok in split_toks])\n",
    "        pbar.update()\n",
    "    dataset_df = pd.DataFrame(data, columns=[\"convID\", \"Utterance\"])    \n",
    "    return dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = preprocess_huggingface_icc(cha_paths, seed=GLOBAL_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(PROCESSED_DATA_DIR,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df.to_csv(os.path.join(PROCESSED_DATA_DIR,\"train\")+\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / Val Datasets \n",
    "\n",
    "Julia originally segmented the train and val. *.cha files into separate folders. Here, we process them to convert them into the appropriate format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_paths = {\n",
    "    \"train\" :os.path.join(PROJECT_ROOT_DIR,\"data\", \"raw\", DATASET_NAME, \"train\"), \n",
    "    \"validation\" : os.path.join(PROJECT_ROOT_DIR,\"data\", \"raw\", DATASET_NAME, \"validation\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(PROCESSED_DATA_DIR,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_DATASET = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for dataset_name, path in dataset_paths.items():\n",
    "    # NOTE: Each of these files represents a single conversation. \n",
    "    cha_paths = glob.glob(\"{}/*.cha\".format(path))\n",
    "    if SMALL_DATASET:\n",
    "        cha_paths = cha_paths[:SMALL_DATASET_SIZE]\n",
    "    dataset_df = preprocess_huggingface_icc(cha_paths, seed=GLOBAL_SEED)\n",
    "    dataset_df.to_csv(os.path.join(PROCESSED_DATA_DIR,dataset_name)+\".csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('gpt_proj')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8226f9d8233b889d814e65b178e86bbabe6dbb1167ee6423a6522d592207fe9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

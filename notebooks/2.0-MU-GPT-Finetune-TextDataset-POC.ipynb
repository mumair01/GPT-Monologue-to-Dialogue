{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "\n",
    "Proof of concept notebook for finetuning GPT using HuggingFace and Torch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download libraries for environment. \n",
    "\n",
    "import sys \n",
    "import os \n",
    "\n",
    "# Env. vars to check if the notebook is running on colab, kaggle etc. \n",
    "IS_COLAB = \"google.colab\" in sys.modules \n",
    "IS_KAGGLE = \"kaggle_secrets\" in sys.modules \n",
    "IS_LOCAL = not (IS_COLAB or IS_KAGGLE)\n",
    "\n",
    "if IS_COLAB:\n",
    "    # Install the packages \n",
    "    %pip install -q -U tensorflow-addons\n",
    "    %pip install -q -U transformers\n",
    "    %pip install -q -U datasets\n",
    "    print(\"You can safely ignore the package incompatibility errors.\")\n",
    "    # Mount the drive \n",
    "    from google.colab import drive \n",
    "    drive.mount(\"/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "\n",
    "import random \n",
    "import shutil \n",
    "# Scikit-Learn â‰¥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "\n",
    "# Pytorch imports \n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Others \n",
    "import glob \n",
    "\n",
    "# Transformers \n",
    "import transformers \n",
    "from transformers import TextDataset,DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments,AutoModelWithLMHead\n",
    "from transformers import AutoTokenizer\n",
    "import datasets \n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --  Set environment global vars. \n",
    "\n",
    "# Shared env. vars. \n",
    "GLOBAL_SEED = 42 \n",
    "IS_CUDA_ENV = torch.cuda.is_available()\n",
    "GLOBAL_DEVICE = torch.device('cuda') if IS_CUDA_ENV else torch.device('cpu')\n",
    "SET_SEED = True # If true, sets the global seeds for this notebook. \n",
    "\n",
    "if IS_LOCAL:\n",
    "    SMALL_MODEL = True if not IS_CUDA_ENV else False # Use a small dataset if no cuda env. \n",
    "\n",
    "if IS_COLAB:\n",
    "    SMALL_MODEL = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring env. \n",
    "if SET_SEED:\n",
    "    # to make this notebook's output stable across runs\n",
    "    np.random.seed(GLOBAL_SEED) \n",
    "    torch.manual_seed(GLOBAL_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/muhammadumair/Documents/Repositories/mumair01-repos/GPT-Monologue-to-Dialogue/data/datasets/processed/ICC/julia_dissertation'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Project Paths\n",
    "NOTEBOOK_NAME = \"2.0-MU-GPT-Finetune-TextDataset-POC\"\n",
    "PROJECT_ROOT_DIR = \"/Users/muhammadumair/Documents/Repositories/mumair01-repos/GPT-Monologue-to-Dialogue\" \n",
    "# --- Input data dirs. \n",
    "DATASET_NAME = \"ICC/julia_dissertation\"\n",
    "DATASET_TYPE = \"csv\"\n",
    "PROCESSED_DATA_DIR = os.path.join(PROJECT_ROOT_DIR,\"data\",\"datasets\", \"processed\", DATASET_NAME)\n",
    "\n",
    "# --- Result dirs. \n",
    "SAVE_MODEL_DIR = os.path.join(PROJECT_ROOT_DIR,\"data\",\"models\",NOTEBOOK_NAME)\n",
    "REPORTS_DIR = os.path.join(PROJECT_ROOT_DIR,\"data\",\"reports\",NOTEBOOK_NAME)\n",
    "\n",
    "PROCESSED_DATA_DIR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning HuggingFace GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NOTE: The below should be the same in the dataset - assuming there are 2 speakers! \n",
    "SPEAKER_1_TOKEN = \"<SP1>\"\n",
    "SPEAKER_2_TOKEN = \"<SP2>\"\n",
    "CONV_START_TOKEN = \"<START>\"\n",
    "CONV_END_TOKEN = \"<END>\"\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "EOS_TOKEN = \"<|endoftext|>\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(REPORTS_DIR, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer vars. \n",
    "\n",
    "TOKENIZER_CHECKPOINT = \"gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /Users/muhammadumair/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.20.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/gpt2/resolve/main/vocab.json from cache at /Users/muhammadumair/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
      "loading file https://huggingface.co/gpt2/resolve/main/merges.txt from cache at /Users/muhammadumair/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/gpt2/resolve/main/tokenizer.json from cache at /Users/muhammadumair/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
      "loading file https://huggingface.co/gpt2/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/gpt2/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/gpt2/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /Users/muhammadumair/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.20.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer with special tokens defined. \n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    TOKENIZER_CHECKPOINT, \n",
    "    pad_token = PAD_TOKEN, \n",
    "    eos_token = EOS_TOKEN, \n",
    "    additional_special_tokens=(\n",
    "        SPEAKER_1_TOKEN, SPEAKER_2_TOKEN, CONV_START_TOKEN, \n",
    "        CONV_END_TOKEN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in /Users/muhammadumair/Documents/Repositories/mumair01-repos/GPT-Monologue-to-Dialogue/data/models/2.0-MU-GPT-Finetune-TextDataset-POC/tokenizer/tokenizer_config.json\n",
      "Special tokens file saved in /Users/muhammadumair/Documents/Repositories/mumair01-repos/GPT-Monologue-to-Dialogue/data/models/2.0-MU-GPT-Finetune-TextDataset-POC/tokenizer/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/Users/muhammadumair/Documents/Repositories/mumair01-repos/GPT-Monologue-to-Dialogue/data/models/2.0-MU-GPT-Finetune-TextDataset-POC/tokenizer/tokenizer_config.json',\n",
       " '/Users/muhammadumair/Documents/Repositories/mumair01-repos/GPT-Monologue-to-Dialogue/data/models/2.0-MU-GPT-Finetune-TextDataset-POC/tokenizer/special_tokens_map.json',\n",
       " '/Users/muhammadumair/Documents/Repositories/mumair01-repos/GPT-Monologue-to-Dialogue/data/models/2.0-MU-GPT-Finetune-TextDataset-POC/tokenizer/vocab.json',\n",
       " '/Users/muhammadumair/Documents/Repositories/mumair01-repos/GPT-Monologue-to-Dialogue/data/models/2.0-MU-GPT-Finetune-TextDataset-POC/tokenizer/merges.txt',\n",
       " '/Users/muhammadumair/Documents/Repositories/mumair01-repos/GPT-Monologue-to-Dialogue/data/models/2.0-MU-GPT-Finetune-TextDataset-POC/tokenizer/added_tokens.json',\n",
       " '/Users/muhammadumair/Documents/Repositories/mumair01-repos/GPT-Monologue-to-Dialogue/data/models/2.0-MU-GPT-Finetune-TextDataset-POC/tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the tokenizer after adding new tokens \n",
    "tokenizer.save_pretrained(os.path.join(SAVE_MODEL_DIR,\"tokenizer\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file /Users/muhammadumair/Documents/Repositories/mumair01-repos/GPT-Monologue-to-Dialogue/data/models/2.0-MU-GPT-Finetune-TextDataset-POC/tokenizer/vocab.json\n",
      "loading file /Users/muhammadumair/Documents/Repositories/mumair01-repos/GPT-Monologue-to-Dialogue/data/models/2.0-MU-GPT-Finetune-TextDataset-POC/tokenizer/merges.txt\n",
      "loading file /Users/muhammadumair/Documents/Repositories/mumair01-repos/GPT-Monologue-to-Dialogue/data/models/2.0-MU-GPT-Finetune-TextDataset-POC/tokenizer/tokenizer.json\n",
      "loading file /Users/muhammadumair/Documents/Repositories/mumair01-repos/GPT-Monologue-to-Dialogue/data/models/2.0-MU-GPT-Finetune-TextDataset-POC/tokenizer/added_tokens.json\n",
      "loading file /Users/muhammadumair/Documents/Repositories/mumair01-repos/GPT-Monologue-to-Dialogue/data/models/2.0-MU-GPT-Finetune-TextDataset-POC/tokenizer/special_tokens_map.json\n",
      "loading file /Users/muhammadumair/Documents/Repositories/mumair01-repos/GPT-Monologue-to-Dialogue/data/models/2.0-MU-GPT-Finetune-TextDataset-POC/tokenizer/tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='/Users/muhammadumair/Documents/Repositories/mumair01-repos/GPT-Monologue-to-Dialogue/data/models/2.0-MU-GPT-Finetune-TextDataset-POC/tokenizer', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<PAD>', 'additional_special_tokens': ['<SP1>', '<SP2>', '<START>', '<END>']})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = tokenizer.from_pretrained(os.path.join(SAVE_MODEL_DIR,\"tokenizer\"))\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data collator, which is responsible for creating batches from the\n",
    "# datasets during training.\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, \n",
    "    mlm=False, \n",
    "    return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': '/Users/muhammadumair/Documents/Repositories/mumair01-repos/GPT-Monologue-to-Dialogue/data/datasets/processed/ICC/julia_dissertation/train.txt',\n",
       " 'validation': '/Users/muhammadumair/Documents/Repositories/mumair01-repos/GPT-Monologue-to-Dialogue/data/datasets/processed/ICC/julia_dissertation/validation.txt'}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# NOTE: Loading the text files for the TextDataset class instead of the csv files. \n",
    "dataset_paths = glob.glob(\"{}/*.txt\".format(PROCESSED_DATA_DIR))\n",
    "dataset_paths = {os.path.splitext(os.path.basename(p))[0] : p for p in dataset_paths}\n",
    "# Only keep the required keys / verify that they exist.\n",
    "dataset_paths = {k : dataset_paths[k] for k in ('train','validation')} \n",
    "dataset_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading features from cached file /Users/muhammadumair/Documents/Repositories/mumair01-repos/GPT-Monologue-to-Dialogue/data/datasets/processed/ICC/julia_dissertation/cached_lm_GPT2TokenizerFast_128_train.txt [took 0.004 s]\n",
      "Loading features from cached file /Users/muhammadumair/Documents/Repositories/mumair01-repos/GPT-Monologue-to-Dialogue/data/datasets/processed/ICC/julia_dissertation/cached_lm_GPT2TokenizerFast_128_validation.txt [took 0.017 s]\n"
     ]
    }
   ],
   "source": [
    "# NOTE: The TextDataset is deprecated - please see the next cell for the updated \n",
    "# method of creating the text dataset. Note that this is still included in \n",
    "# case the new approach does not work. \n",
    "\n",
    "####  UNCOMMENT IF NEEDED ######\n",
    "\n",
    "train_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=dataset_paths[\"train\"],\n",
    "    block_size=128)\n",
    "validation_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=dataset_paths[\"validation\"],\n",
    "    block_size=128)\n",
    "\n",
    "#################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50262"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([50260,   198, 50258,   428,   318,  1257,   220, 50258,   198, 50259,\n",
       "          428,   318, 35607,   502,   503,  2728,  1312,  1254,   588,   545,\n",
       "         2045,   379,   588,   616, 14580,   220, 50259,   198, 50258,   547,\n",
       "         8066,  1716,  3881,  5788,   220, 50258,   198, 50259,  1312,  1254,\n",
       "          588,   545,   588,   287,   262,  2003,   428,   318, 38598,   502,\n",
       "          503,   220, 50259,   198, 50258, 24486,   993,   663,   588,   340,\n",
       "         3073,   588,   340,   714,   307,   257, 10162,   475,   663,   407,\n",
       "          220, 50258,   198, 50259,  1312,  1254,   588,  1312,   588,   340,\n",
       "          318,   257, 10162,   588,   545,  1107, 24069,  1312, 17666,   892,\n",
       "          547,  2035,   588,   287,   262,  1181,   284,  5412,   220, 50259,\n",
       "          198, 50258,  7926, 10194,  1312,   760,  1312,  7360,   423,   257,\n",
       "        50082,   290,   545,   290,   588,   304,    86,  8788,   644,   373,\n",
       "         1312,  2282,   878,   356,  6807,   287,   994,   220])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TextDataset is loading the tokenized version of the utterances\n",
    "# NOTE: It includes the newlines in the data as tokens. \n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<SP1>', 546)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(train_dataset[0][7]), len(tokenizer.decode(train_dataset[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model / Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CHECKPOINT = \"distilgpt2\" if SMALL_MODEL else \"gpt2-large\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilgpt2/resolve/main/config.json from cache at /Users/muhammadumair/.cache/huggingface/transformers/f985248d2791fcff97732e4ee263617adec1edb5429a2b8421734c6d14e39bee.422318838d1ec4e061efb4ea29671cb2a044e244dc69229682bebd7cacc81631\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"distilgpt2\",\n",
      "  \"_num_labels\": 1,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 6,\n",
      "  \"n_positions\": 1024,\n",
      "  \"pad_token_id\": 50257,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.20.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilgpt2/resolve/main/pytorch_model.bin from cache at /Users/muhammadumair/.cache/huggingface/transformers/43a212e83e76bcb07f45be584cf100676bdbbbe9c13f9e5c1c050049143a832f.a83d881ec4d624fd4b5826dd026e315246c48c67504ff91c0500570e291a54ba\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at distilgpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50262, 768)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model \n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_CHECKPOINT, \n",
    "    pad_token_id = tokenizer.pad_token_id, \n",
    "    eos_token_id = tokenizer.eos_token_id\n",
    ")\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining training arguments \n",
    "# NOTE: These args. are taken from Julia's original code - I've found that \n",
    "# deviations drastically change the output during training. \n",
    "training_args = TrainingArguments(\n",
    "        output_dir=SAVE_MODEL_DIR,\n",
    "        overwrite_output_dir=False,\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        eval_steps=200,\n",
    "        save_steps=400,\n",
    "        warmup_steps=300,\n",
    "        prediction_loss_only=True,\n",
    "        evaluation_strategy='epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the trainer \n",
    "# NOTE: Trainer should automatically put the model and dataset to GPU \n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args, \n",
    "    data_collator=data_collator, \n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "396"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clear caches before training \n",
    "torch.cuda.empty_cache()\n",
    "gc.collect() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/muhammadumair/anaconda3/envs/gpt_proj/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 658\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 42\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a274a5c0ba8e4d9597c121fd6401e3ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "############# NOTE: DO NOT RUN UNLESS GPU ENVIRONMENT \n",
    "trainer.train() \n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('gpt_proj')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8226f9d8233b889d814e65b178e86bbabe6dbb1167ee6423a6522d592207fe9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

DATE: 6/19/22

- Progress:
    1. Working on the inference POC notebook for the original GPT code â†’
        Completed 3.0-MU-GPT-SurprisalInference-POC.ipynb notebook.
    2. Creating 4.0-MU-GPT-ICC-preprocess-POC.ipynb notebook for POC of the ICC
        data preprocessing required for the final experiment.

DATE: 6/20/22

- Progress:
    1. Generating train / val / test data from ICC POC notebook.
        - This was done without problems.
    2. Testing GPT finetune POC notebook using the ICC data.
        - This was done with no issues and training started.
    2. Converting notebooks to scripts and running on the HPC.
        - Created script for finetuning GPT and the associated configuration file.
        - Submitted a batch job to the cluster and waiting for results

DATE: 6/21/22
- Progress:
    1. Attempting to fix CUDA runtime errors.
        - Fixed this by using the following configuration:
            - torch == 10.1
            - python=3.8
            - Cuda version = cuda/10.2
    2. Attempting to fix Cuda out of memory error on the first training epoch.
        - Need to experiment with the size of the loaded Dataset.
            - Probably the easiest solution is to just use TextDataset.


DATE: 6/22/22
- Progress:
    1. Attempting to resolve the CUDA memory error issue
        - doing different tests in the POC notebook.
    2. Ran first successful finetuning with custom dataset and limited batch size
        on the cluster.
        - Can't download all the trained models because they are 264 Gigs...

DATE: 6/23/22
- Progress:
    1. Determine how to plot the loss from trained model.

DATE: 6/28/22
- Progress:
    1. Augmenting the preprocessing notebook to ensure that the dataset is
        produced in the correct format for both the TextDataset and the custom
        dataset.
            - Modified the data preprocessing POC notebook to produce text files
            as well.
    2. Finetuning script changes:
        - Created an *_exp finetuning script with the following changes:
            1. tokenizer does not know special tokens.
            2. txt data files are used.
            3. The save /eval steps are changed.
        - Created a copy of all configs on the cluster for ease of separation
        - TODO:
            - If this does not produce the expected result, need to add a mechanism
                that saves the model / evaluates it every epoch (instead of the
                number of steps).
            - If works, create multiple configuration files for multiple use cases.
            - One other problem is that the .csv / .txt data is not in the order
                as Julia's data - there must be a shuffle happening somewhere that
                needs to be fixed.
            - One other approach could be to use the same tokens as were being used
                by Julia.
                    - This might be important because in Julia's experiment, the
                        tokenizer separately tokenizes the speaker identity tokens.
                    - In my version, the tokenizer is
    3. Custom vs. TextDataset:
        - The results for bot the datasets are the same except there are no newlines
        in custom dataset and block size is different.
        - custom dataset is discarding the last chunk.

    4. Running Julia's given script on the data on the HPC.

DATE: 6/29/22
- Progress:
    1. Created new model configs, hpc congifs, and finetuning scripts.
    2. Modified the slurm files to make the output more readable.
    3. Debugging finetuning using refactored code on TextDataset.
        - This works in the POC notebook using the same trainer args as Julia.
    4. Debugging finetuning using refactored code on custom dataset.
<<<<<<< Updated upstream
        - It seems that the tokenizer needs to have the pad + eos tokens set.
DATE: 6/29/22
- Progress:
    1. Creating separate branches for notebooks dev, script dev, and finetuning etc.
        - Adding additional configs and scripts for finetuning.
=======
        - It seems that the tokenizer needs to have the pad + eos tokens set.
>>>>>>> Stashed changes
